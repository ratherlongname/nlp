{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXwgAJELhBTz"
      },
      "source": [
        "!pip -qq install indic-nlp-library    # Import Indic NLP library\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ30cbHzhgiR",
        "outputId": "baa66bff-47e0-4d17-ae07-f0fd09f52581"
      },
      "source": [
        "!unzip translated_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  FInal_dataset.zip\n",
            "replace neg? [y]es, [n]o, [A]ll, [N]one, [r]ename: \n",
            "error:  invalid response [{ENTER}]\n",
            "replace neg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace pos? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace pos? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "FInal_dataset.zip  neg\tpos  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcdQLz6ihSIk",
        "outputId": "24ea71c7-6cdd-447f-8712-5b311fbd5a2f"
      },
      "source": [
        "def word_tok(sents, num_sents):\n",
        "    # Word tokenize each sentence\n",
        "    tokenized_sents = []\n",
        "    for sentence in sents[:num_sents]:\n",
        "        tokenized_sent = indic_tokenize.trivial_tokenize(sentence)\n",
        "        tokenized_sents.append(tokenized_sent)\n",
        "    \n",
        "    # Return array of array of tokens\n",
        "    print(f\"{len(tokenized_sents)} sentences tokenized. First 5 sentences:\\n\\t{tokenized_sents[:5]}\")\n",
        "    return tokenized_sents\n",
        "\n",
        "# Read data into memory\n",
        "pos_data = []\n",
        "with open(\"pos\", 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    pos_data.append(line)\n",
        "\n",
        "pos_lines = len(pos_data)\n",
        "print(f\"{pos_lines} lines of pos reviews read\")\n",
        "\n",
        "neg_data = []\n",
        "with open(\"neg\", 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    neg_data.append(line)\n",
        "\n",
        "neg_lines = len(neg_data)\n",
        "print(f\"{neg_lines} lines of neg reviews read\")\n",
        "\n",
        "\n",
        "# Word tokenize\n",
        "pos_tokenized = word_tok(pos_data, len(pos_data))\n",
        "neg_tokenized = word_tok(neg_data, len(neg_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14143 lines of pos reviews read\n",
            "12882 lines of neg reviews read\n",
            "14143 sentences tokenized. First 5 sentences:\n",
            "\t[['शानदार', ',', 'सभी', 'गायन', ',', 'सभी', 'नाच', 'दावत'], ['इस', 'फिल्म', 'का', 'रहस्य'], ['एक', 'चूसने', 'वाले', 'को', 'कभी', 'भी', 'ब्रेक', 'न', 'दें'], ['सुंदर', ',', 'सार्थक', ',', 'मजाकिया', ',', 'उदास', 'और', 'हमेशा', 'प्रासंगिक'], ['बॉलीवुड', 'के', 'साथ', 'प्यार', 'में', 'पड़ना', 'चाहते', 'हैं', '?', 'यहाँ', 'से', 'प्रारंभ', 'करें']]\n",
            "12882 sentences tokenized. First 5 sentences:\n",
            "\t[['अतिरंजित', 'और', 'कमज़ोर', '।'], ['बस', 'एक', 'दुखद', 'कहानी'], ['यथोचित', 'अच्छी', 'हिंदी', 'फिल्म'], ['यहाँ', 'और', 'वास्तव', 'में', 'काफी', 'भयानक', '!'], ['गाइड', 'ने', '1956', 'की', 'फिल्म', 'बारिशवाला', 'से', 'प्रेरणा', 'ली']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8PBJdamiUDQ"
      },
      "source": [
        "# vectorizer = CountVectorizer()\n",
        "\n",
        "data = pos_data + neg_data\n",
        "targets = [1 for i in range(pos_lines)] + [0 for i in range(neg_lines)]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, targets, test_size=0.25, random_state=42, shuffle=True)\n",
        "\n",
        "# counts = vectorizer.fit_transform(data)\n",
        "# counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPScv3ucnWY3",
        "outputId": "62b1cd80-cb0e-458e-b154-6c040253817d"
      },
      "source": [
        "# transformer = TfidfTransformer(smooth_idf=False)\n",
        "# tfidf = transformer.fit_transform(counts)\n",
        "# tfidf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<27025x3649 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 92440 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy1v2vGqsOyl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gp7VjNkoz-9",
        "outputId": "9d1ba14f-de8c-400e-cc9f-660c3f7f9cc2"
      },
      "source": [
        "# clf = MultinomialNB().fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<14143x2367 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 49024 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb6YTdayo1Yr",
        "outputId": "2f956771-16f5-4895-a0f4-262d2f7cc5bb"
      },
      "source": [
        "svm_clf = Pipeline([\n",
        "                     ('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier(loss='hinge',\n",
        "                                           penalty='l2',\n",
        "                                           alpha=1e-3,\n",
        "                                           random_state=42,\n",
        "                                           max_iter=5,\n",
        "                                           tol=None)),\n",
        "])\n",
        "\n",
        "svm_clf.fit(x_train, y_train)\n",
        "predicted = svm_clf.predict(x_test)\n",
        "np.mean(predicted == y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7584726949829806"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbYh2kNlt3DL",
        "outputId": "df2a084a-e71b-4b55-8cce-69fceff8e839"
      },
      "source": [
        "nb_clf = Pipeline([\n",
        "                     ('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MultinomialNB()),\n",
        "])\n",
        "\n",
        "nb_clf.fit(x_train, y_train)\n",
        "predicted = nb_clf.predict(x_test)\n",
        "np.mean(predicted == y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7433772384194169"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMMmS1nbunAB",
        "outputId": "19eee7c0-d2a1-47e9-898a-b65e774efd57"
      },
      "source": [
        "dt_clf = Pipeline([\n",
        "                     ('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', DecisionTreeClassifier()),\n",
        "])\n",
        "\n",
        "dt_clf.fit(x_train, y_train)\n",
        "predicted = dt_clf.predict(x_test)\n",
        "np.mean(predicted == y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7384934142370875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}